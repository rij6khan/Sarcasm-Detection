{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Predicting Sarcasm using Word2Vec+LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "#!pip install gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "#for word embeddings\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import gensim.downloader as api\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(0); np.random.seed(0); random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#loading datasets\n",
        "train_df = pd.read_csv('train.csv')\n",
        "print('train set has been loaded!')\n",
        "train_df.name = 'train'\n",
        "valid_df = pd.read_csv('valid.csv')\n",
        "print('valid set has been loaded!')\n",
        "valid_df.name = 'valid'\n",
        "test_df = pd.read_csv('test.csv')\n",
        "print('test set has been loaded!')\n",
        "test_df.name = 'test'\n",
        "\n",
        "train_labels = torch.tensor(\n",
        "    train_df['label'].values,\n",
        "    dtype=torch.float32\n",
        ")\n",
        "valid_labels = torch.tensor(\n",
        "    valid_df['label'].values,\n",
        "    dtype=torch.float32\n",
        ")\n",
        "test_labels = torch.tensor(\n",
        "    test_df['label'].values,\n",
        "    dtype=torch.float32\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#looking at features of train_df, valid_df, and test_df:\n",
        "def features(df):\n",
        "  print(f\"Dataset: {df.name}\")\n",
        "  print(f\"Size: {df.shape}\")\n",
        "  print(f\"Columns: {df.columns}\")\n",
        "  print(f\"Null Values: {df.isnull().sum()}\")\n",
        "  print(f\"Head: {df.head()}\\n\")\n",
        "  print(f\"Description:\\n{df.describe()}\\n\")\n",
        "\n",
        "  #longest phrase in dataset\n",
        "  max_len = 0\n",
        "  max_len_phrase = \"\"\n",
        "  for i in range(len(df)):\n",
        "    if len(df['text'][i]) > max_len:\n",
        "      max_len = len(df['text'][i])\n",
        "      max_len_phrase = df['text'][i]\n",
        "  print(f\"Longest Phrase in Dataset: \\n{max_len_phrase}, \\nchar length {max_len}\")\n",
        "\n",
        "  #count of sarcasm vs non-sarcasm\n",
        "  sarcasm_count = df['label'].value_counts()[1]\n",
        "  non_sarcasm_count = df['label'].value_counts()[0]\n",
        "  print(f\"Sarcasm Count: {sarcasm_count}\")\n",
        "  print(f\"Non-Sarcasm Count: {non_sarcasm_count}\")\n",
        "  print(\"-----------------------------\")\n",
        "\n",
        "features(train_df)\n",
        "features(valid_df)\n",
        "features(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the information above, there does not seem to be any null values in any of the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#extract text from sarcasm labels\n",
        "train_text = train_df['text']\n",
        "valid_text = valid_df['text']\n",
        "test_text = test_df['text']\n",
        "\n",
        "#tokenizing them\n",
        "def tokenize(series):\n",
        "    data = []\n",
        "    for text in series:\n",
        "        tokens = [w.lower() for w in word_tokenize(text)]\n",
        "        data.append(tokens)\n",
        "    return data\n",
        "\n",
        "\n",
        "train_tokens = tokenize(train_text)\n",
        "print(train_tokens)\n",
        "valid_tokens = tokenize(valid_text)\n",
        "test_tokens = tokenize(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#getting vocab:\n",
        "def build_vocab(tokens):\n",
        "  counter = Counter(word for sent in tokens for word in sent)\n",
        "  vocab = {\n",
        "    \"<PAD>\": 0,\n",
        "    \"<UNK>\": 1\n",
        "  }\n",
        "  for word in counter:\n",
        "    vocab[word] = len(vocab)\n",
        "\n",
        "  return vocab\n",
        "\n",
        "train_vocab = build_vocab(train_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#converts tokens into numerical indices for model to interpret from\n",
        "#also does padding/truncating\n",
        "def numericalize(tokens, vocab, max_len=30):\n",
        "  seq = [vocab.get(w, vocab[\"<UNK>\"]) for w in tokens]\n",
        "  return seq[:max_len] + [vocab[\"<PAD>\"]] * max(0, max_len - len(seq))\n",
        "\n",
        "train_sequences = torch.tensor([numericalize(s, train_vocab) for s in train_tokens])\n",
        "valid_sequences = torch.tensor([numericalize(s, train_vocab) for s in valid_tokens])\n",
        "test_sequences  = torch.tensor([numericalize(s, train_vocab) for s in test_tokens])\n",
        "\n",
        "#print(train_sequences.max())\n",
        "#print(len(train_vocab))\n",
        "#print(train_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#uncomment only if you dont have GloVe already downloaded\n",
        "#!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "#!unzip -q glove.6B.zip\n",
        "#for mac\n",
        "#!curl -o glove.6B.zip https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "#!unzip -q glove.6B.zip\n",
        "\n",
        "def load_glove_embeddings(glove_path, vocab, embed_dim=50):\n",
        "    embeddings = {}\n",
        "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
        "            embeddings[word] = vector\n",
        "\n",
        "    embedding_matrix = np.zeros((len(vocab), embed_dim))\n",
        "\n",
        "    for word, idx in vocab.items():\n",
        "        if word in embeddings:\n",
        "            embedding_matrix[idx] = embeddings[word]\n",
        "        else:\n",
        "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
        "\n",
        "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "glove_embeddings = load_glove_embeddings(\n",
        "    \"glove.6B.50d.txt\",\n",
        "    train_vocab,\n",
        "    embed_dim=50\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMModelGloVE(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, embed_matrix):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.embed.weight.data.copy_(embed_matrix)\n",
        "        #freeze embedding weights (not needed for training and makes it more stable)\n",
        "        self.embed.weight.requires_grad = False\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True, dropout=0.4)\n",
        "        self.drop1 = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim * 4, 1) #used two poolings, which are both hidden_dim * 2\n",
        "        #self.fc = nn.Linear(hidden_dim * 2, 1) #only if we use one pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "      emb = self.embed(x)\n",
        "      out, (h_n, _) = self.lstm(emb)\n",
        "      avg_pool = out.mean(dim=1) #global average pooling - gets the context of the entire sentence and helps avoid overfitting\n",
        "      max_pool, _ = out.max(dim=1) #global max pooling - helps with feature extraction\n",
        "      h = torch.cat([avg_pool, max_pool], dim=1)\n",
        "      #h = self.drop1(avg_pool)\n",
        "      #h = self.drop1(max_pool)\n",
        "      h = self.drop1(h)\n",
        "      return self.fc(h).squeeze(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#training model (and validating)\n",
        "train_dataset = TensorDataset(train_sequences, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_dataset = TensorDataset(valid_sequences, valid_labels)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "lstm = LSTMModelGloVE(\n",
        "    vocab_size=len(train_vocab),\n",
        "    embed_dim=50,\n",
        "    hidden_dim=64,\n",
        "    embed_matrix=glove_embeddings\n",
        ")\n",
        "lstm.to(device)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "opt = optim.Adam(lstm.parameters(), lr=0.001)\n",
        "\n",
        "#keeping track of validation loss, traning loss, and validation accuracy for visual representation\n",
        "validation_losses = []\n",
        "training_losses = []\n",
        "validation_accuracies = []\n",
        "#early stopping parameters\n",
        "best_loss = float('inf')\n",
        "best_validation_acc = float('-inf')\n",
        "epochs_not_improving = 0\n",
        "\n",
        "#loop\n",
        "for epoch in range(30):\n",
        "  #training\n",
        "  #unfreeze the embedded weights later in loop to avoid overfitting\n",
        "  if epoch == 4:\n",
        "    lstm.embed.weight.requires_grad = True\n",
        "  lstm.train()\n",
        "\n",
        "  total_loss = 0.0\n",
        "  for batch_sequences, batch_labels in train_loader:\n",
        "    batch_sequences = batch_sequences.to(device)\n",
        "    batch_labels = batch_labels.to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    output = lstm(batch_sequences)\n",
        "    loss = loss_fn(output, batch_labels)\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(lstm.parameters(), 1.0) #gradient clipping to avoid exploding gradients\n",
        "    opt.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "  train_loss = total_loss / len(train_loader)\n",
        "\n",
        "  #validating\n",
        "  lstm.eval()\n",
        "  validation_loss = 0.0\n",
        "  preds = []\n",
        "  labels = []\n",
        "  with torch.no_grad():\n",
        "    for batch_sequences, batch_labels in valid_loader:\n",
        "      batch_sequences = batch_sequences.to(device)\n",
        "      batch_labels = batch_labels.to(device)\n",
        "      output = lstm(batch_sequences)\n",
        "      loss = loss_fn(output, batch_labels)\n",
        "      validation_loss += loss.item()\n",
        "      preds.extend((torch.sigmoid(output) > 0.5).float().cpu().numpy())\n",
        "      labels.extend(batch_labels.cpu().numpy())\n",
        "  validation_loss /= len(valid_loader)\n",
        "  validation_accuracy = accuracy_score(labels, preds)\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/30 : Loss {train_loss} | Validation Loss: {validation_loss} | Validation Accuracy {validation_accuracy * 100:.4f}%\")\n",
        "  #either do early stopping or update to more optimal model\n",
        "  if validation_accuracy > best_validation_acc:\n",
        "    best_validation_acc = validation_accuracy\n",
        "    torch.save(lstm.state_dict(), 'model_acc.pth')\n",
        "    print(\"Model_acc saved!\")\n",
        "  if validation_loss < best_loss:\n",
        "    best_loss = validation_loss\n",
        "    torch.save(lstm.state_dict(), 'model.pth')\n",
        "    print(\"Model saved!\")\n",
        "    epochs_not_improving = 0 #reset everytime model improves\n",
        "  else:\n",
        "    epochs_not_improving += 1\n",
        "    if epochs_not_improving >= 7:\n",
        "      print(\"Early stopping!\")\n",
        "      break\n",
        "\n",
        "  #add information to data\n",
        "  training_losses.append(train_loss)\n",
        "  validation_losses.append(validation_loss)\n",
        "  validation_accuracies.append(validation_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#plotting validation loss, validation accuracy, and training loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(validation_losses, label='Validation Loss')\n",
        "plt.plot(validation_accuracies, label='Validation Accuracy')\n",
        "plt.plot(training_losses, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Validation Loss, Validation Accuracy, and Training Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before implementing early stopping, we saw that overfitting happening in the training loop, giving test accuracy of around 80% later on. This still occurs as seen in the above, where the validation loss increases after a few epochs. With this, we ended up with a higher testing accuracy close to 88%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#confusion matrix and error analysis\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "#testing the model\n",
        "test_labels = torch.tensor(test_df['label'].values, dtype=torch.float32).unsqueeze(1)\n",
        "test_dataset = TensorDataset(test_sequences, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "#load most optimal model (lowest validation loss) from training loop\n",
        "lstm.load_state_dict(torch.load('model.pth'))\n",
        "lstm.to(device)\n",
        "lstm.eval()\n",
        "preds = []\n",
        "labels = []\n",
        "with torch.no_grad():\n",
        "  for batch_sequences, batch_labels in test_loader:\n",
        "    batch_sequences = batch_sequences.to(device)\n",
        "\n",
        "    logits = lstm(batch_sequences)\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "    preds.extend((probs > 0.5).float().cpu().numpy())\n",
        "    labels.extend(batch_labels.cpu().numpy())\n",
        "test_accuracy = accuracy_score(labels, preds)\n",
        "print(f\"Test Accuracy with model.pth: {test_accuracy * 100:.4f}%\")\n",
        "print(classification_report(labels, preds, digits=4))\n",
        "conf_matrix = confusion_matrix(labels, preds)\n",
        "print(conf_matrix)\n",
        "sns.heatmap(conf_matrix, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#getting weights of model, embedding, and vocabulary to be used in predict_sarcasm.py\n",
        "torch.save(lstm.state_dict(), 'model_weights.pth')\n",
        "\n",
        "#exporting embedding weights\n",
        "#print(lstm.embed.weight)\n",
        "torch.save(lstm.embed.weight, 'embedding_matrix.pt')\n",
        "\n",
        "#exporting vocabulary\n",
        "#print(train_vocab)\n",
        "torch.save(train_vocab, 'vocab.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load most optimal model (best validation accuracy) from training loop\n",
        "lstm.load_state_dict(torch.load('model_acc.pth'))\n",
        "lstm.to(device)\n",
        "lstm.eval()\n",
        "preds = []\n",
        "labels = []\n",
        "with torch.no_grad():\n",
        "  for batch_sequences, batch_labels in test_loader:\n",
        "    batch_sequences = batch_sequences.to(device)\n",
        "\n",
        "    logits = lstm(batch_sequences)\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "    preds.extend((probs > 0.5).float().cpu().numpy())\n",
        "    labels.extend(batch_labels.cpu().numpy())\n",
        "test_accuracy = accuracy_score(labels, preds)\n",
        "print(f\"Test Accuracy with model_acc.pth: {test_accuracy * 100:.4f}%\")\n",
        "print(classification_report(labels, preds, digits=4))\n",
        "confusion_matrix_acc = confusion_matrix(labels, preds)\n",
        "sns.heatmap(confusion_matrix_acc, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
